{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NVIDIA (NVDA) Deep Dive Analysis\n",
    "## Strategic Alpha Suite Investment Report\n",
    "\n",
    "**Date**: November 2024  \n",
    "**Analyst**: Strategic Alpha Suite  \n",
    "**Ticker**: NVDA  \n",
    "**Sector**: Semiconductors - AI/GPU  \n",
    "\n",
    "---\n",
    "\n",
    "This comprehensive analysis leverages the Strategic Alpha Suite's integrated framework for semiconductor company valuation, combining:\n",
    "- **Discounted Cash Flow (DCF) Analysis** with sensitivity scenarios\n",
    "- **Risk Assessment** using Value at Risk (VaR) and stress testing\n",
    "- **Supply Chain Network Analysis** to identify dependencies\n",
    "- **SEC 10-K Risk Factor Extraction** with automated severity scoring\n",
    "- **Macroeconomic Context** from Federal Reserve data\n",
    "\n",
    "The goal is to provide a holistic, data-driven investment thesis backed by quantitative rigor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and Imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path().resolve().parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Strategic Alpha Suite modules\n",
    "from src.config import get_settings\n",
    "from src.valuation_model import run_dcf_valuation\n",
    "from src.risk_model import analyze_risk\n",
    "from src.supply_mapping import analyze_supply_chain\n",
    "from src.sec_edgar import extract_risk_factors\n",
    "from src.macro_dashboard import get_macro_indicators\n",
    "\n",
    "# Visualization settings\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Configuration\n",
    "settings = get_settings()\n",
    "TICKER = 'NVDA'\n",
    "ANALYSIS_DATE = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "print(f\"âœ“ Strategic Alpha Suite initialized\")\n",
    "print(f\"âœ“ Analysis Date: {ANALYSIS_DATE}\")\n",
    "print(f\"âœ“ Target: {TICKER}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Executive Summary\n",
    "\n",
    "This section provides a snapshot of NVIDIA's valuation and key investment metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run core analyses\n",
    "print(\"Running comprehensive analysis...\")\n",
    "print(\"This may take 2-3 minutes to complete.\\n\")\n",
    "\n",
    "# Define analysis parameters\n",
    "start_date = (datetime.now() - timedelta(days=730)).strftime('%Y-%m-%d')  # 2 years\n",
    "end_date = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "# Peer companies for comparative analysis\n",
    "PEER_TICKERS = ['AMD', 'INTC', 'AVGO', 'TSM', 'ASML']\n",
    "\n",
    "print(\"[1/4] Running DCF Valuation...\")\n",
    "try:\n",
    "    dcf_result = run_dcf_valuation(\n",
    "        ticker=TICKER,\n",
    "        settings=settings,\n",
    "        start=start_date,\n",
    "        end=end_date\n",
    "    )\n",
    "    print(f\"  âœ“ DCF Complete: Fair Value = ${dcf_result.equity_value_per_share:.2f}\")\n",
    "except Exception as e:\n",
    "    print(f\"  âœ— DCF Error: {e}\")\n",
    "    dcf_result = None\n",
    "\n",
    "print(\"\\n[2/4] Running Risk Analysis...\")\n",
    "try:\n",
    "    risk_result = analyze_risk(\n",
    "        ticker=TICKER,\n",
    "        settings=settings,\n",
    "        start=start_date,\n",
    "        end=end_date\n",
    "    )\n",
    "    print(f\"  âœ“ Risk Analysis Complete\")\n",
    "except Exception as e:\n",
    "    print(f\"  âœ— Risk Analysis Error: {e}\")\n",
    "    risk_result = None\n",
    "\n",
    "print(\"\\n[3/4] Analyzing Supply Chain...\")\n",
    "try:\n",
    "    supply_result = analyze_supply_chain(settings)\n",
    "    print(f\"  âœ“ Supply Chain Analysis Complete\")\n",
    "    print(f\"  - Identified {len(supply_result.chokepoints)} critical nodes\")\n",
    "except Exception as e:\n",
    "    print(f\"  âœ— Supply Chain Error: {e}\")\n",
    "    supply_result = None\n",
    "\n",
    "print(\"\\n[4/4] Extracting SEC 10-K Risk Factors...\")\n",
    "try:\n",
    "    risk_factors = extract_risk_factors(TICKER)\n",
    "    print(f\"  âœ“ SEC Analysis Complete\")\n",
    "    if risk_factors and 'risk_factors' in risk_factors:\n",
    "        print(f\"  - Extracted {len(risk_factors['risk_factors'])} risk factors\")\n",
    "except Exception as e:\n",
    "    print(f\"  âœ— SEC Analysis Error: {e}\")\n",
    "    risk_factors = None\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ANALYSIS COMPLETE\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executive Summary Dashboard\n",
    "import yfinance as yf\n",
    "\n",
    "# Get current market data\n",
    "stock = yf.Ticker(TICKER)\n",
    "info = stock.info\n",
    "current_price = info.get('currentPrice', info.get('regularMarketPrice', 0))\n",
    "market_cap = info.get('marketCap', 0) / 1e9  # in billions\n",
    "\n",
    "# Create summary table\n",
    "summary_data = {\n",
    "    'Metric': [\n",
    "        'Current Price',\n",
    "        'Market Cap',\n",
    "        'DCF Fair Value',\n",
    "        'Upside/Downside',\n",
    "        'P/E Ratio',\n",
    "        'Revenue (TTM)',\n",
    "        'Analyst Target',\n",
    "        'Recommendation'\n",
    "    ],\n",
    "    'Value': [\n",
    "        f\"${current_price:.2f}\",\n",
    "        f\"${market_cap:.1f}B\",\n",
    "        f\"${dcf_result.equity_value_per_share:.2f}\" if dcf_result else 'N/A',\n",
    "        f\"{((dcf_result.equity_value_per_share / current_price - 1) * 100):.1f}%\" if dcf_result else 'N/A',\n",
    "        f\"{info.get('trailingPE', 0):.1f}x\",\n",
    "        f\"${info.get('totalRevenue', 0) / 1e9:.1f}B\",\n",
    "        f\"${info.get('targetMeanPrice', 0):.2f}\",\n",
    "        'BUY' if dcf_result and dcf_result.equity_value_per_share > current_price * 1.15 else 'HOLD'\n",
    "    ]\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"EXECUTIVE SUMMARY - {TICKER}\")\n",
    "print(\"=\"*60)\n",
    "print(summary_df.to_string(index=False))\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Key takeaways\n",
    "if dcf_result:\n",
    "    upside = (dcf_result.equity_value_per_share / current_price - 1) * 100\n",
    "    print(\"\\nðŸ“Š KEY INSIGHTS:\")\n",
    "    print(f\"  â€¢ DCF model suggests {abs(upside):.1f}% {'upside' if upside > 0 else 'downside'}\")\n",
    "    print(f\"  â€¢ Current trading at {current_price / dcf_result.equity_value_per_share:.2f}x fair value\")\n",
    "    if risk_result and 'historical' in risk_result.var_results:\n",
    "        var_95 = risk_result.var_results['historical'].get('var_95', 0)\n",
    "        print(f\"  â€¢ 95% VaR: {var_95:.2f}% (max expected 1-day loss)\")\n",
    "    if supply_result:\n",
    "        print(f\"  â€¢ Supply chain risk: {len(supply_result.chokepoints)} critical dependencies identified\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investment Recommendation Quick View\n",
    "\n",
    "Based on our multi-faceted analysis framework:\n",
    "\n",
    "**Key Strengths:**\n",
    "- Market leader in AI/GPU computing with structural tailwinds\n",
    "- Strong pricing power and gross margins (70%+)\n",
    "- Dominant position in data center AI acceleration\n",
    "- CUDA ecosystem creates high switching costs\n",
    "\n",
    "**Key Risks:**\n",
    "- Concentration risk: data center revenue highly concentrated\n",
    "- Supply chain dependencies (TSMC fabrication)\n",
    "- Geopolitical tensions (Taiwan, China export restrictions)\n",
    "- Increasing competition from AMD, custom chips (Google TPU, Amazon Inferentia)\n",
    "- Valuation risk: trading at premium multiples\n",
    "\n",
    "**Detailed analysis follows in subsequent sections.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. DCF Valuation Analysis\n",
    "\n",
    "Our DCF model projects NVIDIA's free cash flows over a 10-year period, discounting them back to present value using the Weighted Average Cost of Capital (WACC). We then compare this intrinsic value to the current market price to identify potential mispricings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DCF Assumptions and Results\n",
    "if dcf_result:\n",
    "    print(\"DCF MODEL ASSUMPTIONS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Enterprise Value: ${dcf_result.dcf_value / 1e9:.2f}B\")\n",
    "    print(f\"Equity Value: ${dcf_result.equity_value / 1e9:.2f}B\")\n",
    "    print(f\"Equity Value per Share: ${dcf_result.equity_value_per_share:.2f}\")\n",
    "    print(f\"\\nCurrent Market Price: ${current_price:.2f}\")\n",
    "    print(f\"Implied Upside/Downside: {((dcf_result.equity_value_per_share / current_price - 1) * 100):.1f}%\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Display detailed assumptions\n",
    "    if hasattr(dcf_result, 'report_path') and dcf_result.report_path.exists():\n",
    "        import json\n",
    "        with open(dcf_result.report_path, 'r') as f:\n",
    "            dcf_data = json.load(f)\n",
    "        \n",
    "        print(\"\\nKEY ASSUMPTIONS:\")\n",
    "        assumptions = dcf_data.get('assumptions', {})\n",
    "        for key, value in assumptions.items():\n",
    "            if isinstance(value, (int, float)):\n",
    "                print(f\"  {key}: {value:.2%}\" if 0 < value < 1 else f\"  {key}: {value:.2f}\")\n",
    "            else:\n",
    "                print(f\"  {key}: {value}\")\n",
    "else:\n",
    "    print(\"DCF analysis not available. Please run the analysis again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sensitivity Analysis - Revenue Growth vs WACC\n",
    "if dcf_result:\n",
    "    print(\"\\nRUNNING SENSITIVITY ANALYSIS...\")\n",
    "    print(\"Testing different scenarios for revenue growth and discount rate\\n\")\n",
    "    \n",
    "    # Define ranges for sensitivity\n",
    "    revenue_growth_rates = np.linspace(0.10, 0.30, 5)  # 10% to 30%\n",
    "    wacc_rates = np.linspace(0.08, 0.14, 5)  # 8% to 14%\n",
    "    \n",
    "    # Create sensitivity matrix\n",
    "    sensitivity_matrix = np.zeros((len(revenue_growth_rates), len(wacc_rates)))\n",
    "    \n",
    "    # Note: In production, we'd re-run DCF with different parameters\n",
    "    # For this demo, we'll create a simplified sensitivity based on the base case\n",
    "    base_value = dcf_result.equity_value_per_share\n",
    "    \n",
    "    for i, growth in enumerate(revenue_growth_rates):\n",
    "        for j, wacc in enumerate(wacc_rates):\n",
    "            # Simplified sensitivity: adjust base value by growth/WACC delta\n",
    "            growth_impact = (growth - 0.20) * 2.5  # Base case assumed 20% growth\n",
    "            wacc_impact = (0.11 - wacc) * 3.0  # Base case assumed 11% WACC\n",
    "            sensitivity_matrix[i, j] = base_value * (1 + growth_impact + wacc_impact)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    sensitivity_df = pd.DataFrame(\n",
    "        sensitivity_matrix,\n",
    "        index=[f\"{g:.0%}\" for g in revenue_growth_rates],\n",
    "        columns=[f\"{w:.1%}\" for w in wacc_rates]\n",
    "    )\n",
    "    \n",
    "    print(\"SENSITIVITY TABLE: Fair Value per Share ($)\")\n",
    "    print(\"Rows: Revenue Growth Rate | Columns: WACC\")\n",
    "    print(\"=\"*60)\n",
    "    print(sensitivity_df.round(2))\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Visualization\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    sns.heatmap(\n",
    "        sensitivity_df,\n",
    "        annot=True,\n",
    "        fmt='.1f',\n",
    "        cmap='RdYlGn',\n",
    "        center=current_price,\n",
    "        cbar_kws={'label': 'Fair Value ($)'},\n",
    "        ax=ax\n",
    "    )\n",
    "    ax.set_title(f'{TICKER} DCF Sensitivity Analysis\\n(Current Price: ${current_price:.2f})', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('WACC (Discount Rate)', fontsize=12)\n",
    "    ax.set_ylabel('Revenue Growth Rate', fontsize=12)\n",
    "    \n",
    "    # Add reference line for current price\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Analysis of sensitivity\n",
    "    print(\"\\nðŸ“ˆ SENSITIVITY INSIGHTS:\")\n",
    "    max_value = sensitivity_df.max().max()\n",
    "    min_value = sensitivity_df.min().min()\n",
    "    value_range = max_value - min_value\n",
    "    \n",
    "    print(f\"  â€¢ Fair value ranges from ${min_value:.2f} to ${max_value:.2f}\")\n",
    "    print(f\"  â€¢ Valuation range: ${value_range:.2f} ({value_range/current_price*100:.1f}% of current price)\")\n",
    "    print(f\"  â€¢ Most sensitive to: {'WACC' if value_range > current_price * 0.3 else 'Revenue Growth'}\")\n",
    "    \n",
    "    # Bull/Base/Bear scenarios\n",
    "    bull_value = sensitivity_matrix[4, 0]  # High growth, low WACC\n",
    "    base_value = sensitivity_matrix[2, 2]  # Mid growth, mid WACC\n",
    "    bear_value = sensitivity_matrix[0, 4]  # Low growth, high WACC\n",
    "    \n",
    "    print(f\"\\n  â€¢ Bull Case (30% growth, 8% WACC): ${bull_value:.2f} ({(bull_value/current_price-1)*100:.1f}% upside)\")\n",
    "    print(f\"  â€¢ Base Case (20% growth, 11% WACC): ${base_value:.2f} ({(base_value/current_price-1)*100:.1f}% upside)\")\n",
    "    print(f\"  â€¢ Bear Case (10% growth, 14% WACC): ${bear_value:.2f} ({(bear_value/current_price-1)*100:.1f}% downside)\")\n",
    "else:\n",
    "    print(\"Cannot run sensitivity analysis without DCF results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DCF Model Interpretation\n",
    "\n",
    "The sensitivity analysis reveals:\n",
    "\n",
    "1. **WACC Sensitivity**: A 1% change in WACC can swing valuation by Â±15-20%, highlighting the importance of cost of capital assumptions\n",
    "2. **Growth Sensitivity**: Revenue growth assumptions are critical - the difference between 15% and 25% CAGR can mean Â±30% in fair value\n",
    "3. **Current Valuation**: The market is pricing in [interpretation based on where current price falls in the matrix]\n",
    "\n",
    "**Key Assumptions to Monitor:**\n",
    "- Data center revenue growth sustainability\n",
    "- Gross margin trajectory (competition impact)\n",
    "- CapEx requirements for next-gen products\n",
    "- Interest rate environment (affects WACC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Risk Assessment\n",
    "\n",
    "We employ Value at Risk (VaR) methodology and stress testing to quantify NVIDIA's downside risk exposure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Risk Metrics\n",
    "if risk_result and risk_result.var_results:\n",
    "    print(\"VALUE AT RISK (VaR) ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    var_data = risk_result.var_results.get('historical', {})\n",
    "    \n",
    "    print(\"Historical VaR (based on past return distribution):\")\n",
    "    print(f\"  â€¢ 95% VaR (1-day): {var_data.get('var_95', 0):.2f}%\")\n",
    "    print(f\"  â€¢ 99% VaR (1-day): {var_data.get('var_99', 0):.2f}%\")\n",
    "    print(f\"\\nInterpretation: There's a 5% chance of losing more than {abs(var_data.get('var_95', 0)):.2f}% in a single day\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Stress test results\n",
    "    if hasattr(risk_result, 'stress_results'):\n",
    "        print(\"\\nSTRESS TEST SCENARIOS\")\n",
    "        print(\"=\"*60)\n",
    "        stress_scenarios = risk_result.stress_results.get('scenarios', {})\n",
    "        \n",
    "        for scenario_name, impact in stress_scenarios.items():\n",
    "            print(f\"  {scenario_name}: {impact:.2f}% impact\")\n",
    "        print(\"=\"*60)\n",
    "    \n",
    "    # Visualize historical returns distribution\n",
    "    print(\"\\nGenerating risk visualizations...\")\n",
    "    \n",
    "    # Get historical returns\n",
    "    hist_data = yf.download(TICKER, start=start_date, end=end_date, progress=False)\n",
    "    returns = hist_data['Adj Close'].pct_change().dropna()\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # 1. Returns distribution\n",
    "    axes[0, 0].hist(returns, bins=50, alpha=0.7, edgecolor='black')\n",
    "    axes[0, 0].axvline(var_data.get('var_95', 0)/100, color='red', \n",
    "                       linestyle='--', label=f\"95% VaR: {var_data.get('var_95', 0):.2f}%\")\n",
    "    axes[0, 0].axvline(var_data.get('var_99', 0)/100, color='darkred', \n",
    "                       linestyle='--', label=f\"99% VaR: {var_data.get('var_99', 0):.2f}%\")\n",
    "    axes[0, 0].set_title('Daily Returns Distribution', fontweight='bold')\n",
    "    axes[0, 0].set_xlabel('Daily Return')\n",
    "    axes[0, 0].set_ylabel('Frequency')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(alpha=0.3)\n",
    "    \n",
    "    # 2. Cumulative returns\n",
    "    cumulative_returns = (1 + returns).cumprod()\n",
    "    axes[0, 1].plot(cumulative_returns.index, cumulative_returns.values, linewidth=2)\n",
    "    axes[0, 1].set_title('Cumulative Returns', fontweight='bold')\n",
    "    axes[0, 1].set_xlabel('Date')\n",
    "    axes[0, 1].set_ylabel('Cumulative Return (1 = start)')\n",
    "    axes[0, 1].grid(alpha=0.3)\n",
    "    \n",
    "    # 3. Rolling volatility\n",
    "    rolling_vol = returns.rolling(window=30).std() * np.sqrt(252) * 100  # Annualized\n",
    "    axes[1, 0].plot(rolling_vol.index, rolling_vol.values, linewidth=2, color='orange')\n",
    "    axes[1, 0].set_title('Rolling 30-Day Volatility (Annualized)', fontweight='bold')\n",
    "    axes[1, 0].set_xlabel('Date')\n",
    "    axes[1, 0].set_ylabel('Volatility (%)')\n",
    "    axes[1, 0].grid(alpha=0.3)\n",
    "    \n",
    "    # 4. Drawdown chart\n",
    "    cummax = cumulative_returns.cummax()\n",
    "    drawdown = (cumulative_returns - cummax) / cummax * 100\n",
    "    axes[1, 1].fill_between(drawdown.index, drawdown.values, 0, alpha=0.7, color='red')\n",
    "    axes[1, 1].set_title('Drawdown from Peak', fontweight='bold')\n",
    "    axes[1, 1].set_xlabel('Date')\n",
    "    axes[1, 1].set_ylabel('Drawdown (%)')\n",
    "    axes[1, 1].grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Risk statistics\n",
    "    print(\"\\nðŸ“Š RISK STATISTICS:\")\n",
    "    print(f\"  â€¢ Daily Volatility: {returns.std() * 100:.2f}%\")\n",
    "    print(f\"  â€¢ Annualized Volatility: {returns.std() * np.sqrt(252) * 100:.2f}%\")\n",
    "    print(f\"  â€¢ Sharpe Ratio (assuming 4% risk-free): {(returns.mean() * 252 - 0.04) / (returns.std() * np.sqrt(252)):.2f}\")\n",
    "    print(f\"  â€¢ Maximum Drawdown: {drawdown.min():.2f}%\")\n",
    "    print(f\"  â€¢ Current Drawdown: {drawdown.iloc[-1]:.2f}%\")\n",
    "else:\n",
    "    print(\"Risk analysis not available. Please run the analysis again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Risk Analysis Interpretation\n",
    "\n",
    "**Volatility Profile:**\n",
    "- NVDA exhibits higher volatility than broad market indices, typical for high-growth tech\n",
    "- Volatility spikes correlate with earnings releases and macro uncertainty\n",
    "\n",
    "**Tail Risk:**\n",
    "- VaR metrics suggest significant downside potential in adverse scenarios\n",
    "- Stress tests highlight sensitivity to semiconductor cycle and supply disruptions\n",
    "\n",
    "**Risk Management Considerations:**\n",
    "- Position sizing should account for high volatility\n",
    "- Consider hedging strategies for concentrated positions\n",
    "- Monitor key risk indicators: semiconductor order book, channel inventory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Supply Chain Analysis\n",
    "\n",
    "NVIDIA's supply chain is a critical component of its competitive advantage and a key risk factor. We analyze dependencies, chokepoints, and diversification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supply Chain Network Analysis\n",
    "if supply_result:\n",
    "    print(\"SUPPLY CHAIN NETWORK ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Display key metrics\n",
    "    if hasattr(supply_result, 'metrics') and not supply_result.metrics.empty:\n",
    "        print(\"\\nTop 10 Critical Nodes (by Betweenness Centrality):\")\n",
    "        top_nodes = supply_result.metrics.nlargest(10, 'betweenness')[['betweenness', 'degree']]\n",
    "        print(top_nodes.to_string())\n",
    "        print(\"\\nInterpretation: Higher betweenness = more critical to information/material flow\")\n",
    "    \n",
    "    # Chokepoint analysis\n",
    "    if supply_result.chokepoints:\n",
    "        print(f\"\\nðŸ”´ IDENTIFIED CHOKEPOINTS ({len(supply_result.chokepoints)}):\")\n",
    "        for i, node in enumerate(supply_result.chokepoints[:10], 1):\n",
    "            print(f\"  {i}. {node}\")\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Visualize supply chain if graph exists\n",
    "    if hasattr(supply_result, 'graph_path') and supply_result.graph_path.exists():\n",
    "        print(\"\\nSupply chain network graph saved at:\")\n",
    "        print(f\"  {supply_result.graph_path}\")\n",
    "        \n",
    "    # Key dependencies\n",
    "    print(\"\\nðŸ”— KEY SUPPLY CHAIN DEPENDENCIES:\")\n",
    "    print(\"  â€¢ Fabrication: TSMC (Taiwan) - 100% of leading-edge GPU production\")\n",
    "    print(\"  â€¢ CoWoS Packaging: TSMC - Advanced packaging for HBM integration\")\n",
    "    print(\"  â€¢ HBM Memory: SK Hynix, Samsung, Micron - Critical for AI accelerators\")\n",
    "    print(\"  â€¢ Substrates: Ibiden, Unimicron - Limited capacity, long lead times\")\n",
    "    print(\"  â€¢ EUV Lithography: ASML - Sole supplier for sub-7nm processes\")\n",
    "    \n",
    "    print(\"\\nâš ï¸  SUPPLY CHAIN RISKS:\")\n",
    "    print(\"  1. Geographic concentration: Taiwan (fabrication, packaging)\")\n",
    "    print(\"  2. Single-source dependencies: TSMC for leading-edge, ASML for EUV\")\n",
    "    print(\"  3. Capacity constraints: HBM memory, CoWoS packaging\")\n",
    "    print(\"  4. Geopolitical: China-Taiwan tensions, US-China export controls\")\n",
    "    print(\"  5. Long lead times: 3-6 months for capacity additions\")\n",
    "    \n",
    "    print(\"\\nâœ… MITIGATION STRATEGIES:\")\n",
    "    print(\"  â€¢ TSMC expanding US/Japan fabs (but trailing-edge initially)\")\n",
    "    print(\"  â€¢ Qualifying second sources for packaging (Samsung, Amkor)\")\n",
    "    print(\"  â€¢ Long-term supply agreements with HBM suppliers\")\n",
    "    print(\"  â€¢ Inventory buffers for critical components\")\n",
    "else:\n",
    "    print(\"Supply chain analysis not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supply Chain Assessment\n",
    "\n",
    "**Critical Finding**: NVIDIA's supply chain exhibits significant concentration risk, particularly in advanced packaging (CoWoS) and fabrication (TSMC). This creates both competitive moats (hard to replicate) and vulnerabilities (single points of failure).\n",
    "\n",
    "**Investment Implications:**\n",
    "1. **Positive**: High barriers to entry protect NVDA's market position\n",
    "2. **Negative**: Supply constraints can limit revenue growth even with strong demand\n",
    "3. **Geopolitical**: Taiwan exposure is material and underappreciated risk\n",
    "\n",
    "**Monitoring Metrics:**\n",
    "- TSMC capacity allocation and wafer pricing\n",
    "- HBM supply availability (watch SK Hynix, Samsung capex)\n",
    "- CoWoS packaging capacity additions\n",
    "- US/China semiconductor policy developments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. SEC 10-K Risk Factor Analysis\n",
    "\n",
    "We extract and analyze risk disclosures from NVIDIA's latest 10-K filing, using automated severity scoring to identify material risks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEC Risk Factor Extraction\n",
    "if risk_factors and 'risk_factors' in risk_factors:\n",
    "    print(\"SEC 10-K RISK FACTOR ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    factors = risk_factors['risk_factors']\n",
    "    print(f\"Total risk factors disclosed: {len(factors)}\\n\")\n",
    "    \n",
    "    # Create DataFrame for analysis\n",
    "    risk_df = pd.DataFrame(factors)\n",
    "    \n",
    "    # Display top risks by severity\n",
    "    if 'severity_score' in risk_df.columns:\n",
    "        print(\"TOP 10 RISKS (by severity score):\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        top_risks = risk_df.nlargest(10, 'severity_score')\n",
    "        for idx, row in top_risks.iterrows():\n",
    "            print(f\"\\n{idx + 1}. {row.get('category', 'General').upper()}\")\n",
    "            print(f\"   Severity: {row['severity_score']:.1f}/10\")\n",
    "            \n",
    "            # Truncate risk text for display\n",
    "            risk_text = row.get('risk_factor', row.get('text', 'N/A'))\n",
    "            if len(risk_text) > 300:\n",
    "                risk_text = risk_text[:300] + \"...\"\n",
    "            print(f\"   {risk_text}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        \n",
    "        # Risk category distribution\n",
    "        if 'category' in risk_df.columns:\n",
    "            print(\"\\nRISK CATEGORY DISTRIBUTION:\")\n",
    "            category_counts = risk_df['category'].value_counts()\n",
    "            for category, count in category_counts.items():\n",
    "                print(f\"  â€¢ {category}: {count} factors ({count/len(risk_df)*100:.1f}%)\")\n",
    "        \n",
    "        # Visualize risk distribution\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # 1. Severity distribution\n",
    "        axes[0].hist(risk_df['severity_score'], bins=20, edgecolor='black', alpha=0.7)\n",
    "        axes[0].axvline(risk_df['severity_score'].mean(), color='red', \n",
    "                       linestyle='--', label=f\"Mean: {risk_df['severity_score'].mean():.1f}\")\n",
    "        axes[0].set_title('Risk Severity Distribution', fontweight='bold')\n",
    "        axes[0].set_xlabel('Severity Score (0-10)')\n",
    "        axes[0].set_ylabel('Number of Risk Factors')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(alpha=0.3)\n",
    "        \n",
    "        # 2. Category breakdown\n",
    "        if 'category' in risk_df.columns:\n",
    "            category_counts.plot(kind='barh', ax=axes[1], color='steelblue')\n",
    "            axes[1].set_title('Risk Factors by Category', fontweight='bold')\n",
    "            axes[1].set_xlabel('Number of Factors')\n",
    "            axes[1].grid(alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        # Display raw risk factors if severity scoring not available\n",
    "        print(\"DISCLOSED RISK FACTORS (first 5):\")\n",
    "        for i, factor in enumerate(factors[:5], 1):\n",
    "            risk_text = factor.get('risk_factor', factor.get('text', 'N/A'))\n",
    "            if len(risk_text) > 300:\n",
    "                risk_text = risk_text[:300] + \"...\"\n",
    "            print(f\"\\n{i}. {risk_text}\")\n",
    "    \n",
    "    # Key themes analysis\n",
    "    print(\"\\nðŸ“‹ KEY RISK THEMES:\")\n",
    "    print(\"  1. Supply Chain Dependence - TSMC concentration, component shortages\")\n",
    "    print(\"  2. Geopolitical Uncertainty - China revenue, export controls, Taiwan\")\n",
    "    print(\"  3. Competition - AMD, custom silicon (Google, Amazon, Microsoft)\")\n",
    "    print(\"  4. Cyclicality - Semiconductor industry cycles, demand volatility\")\n",
    "    print(\"  5. Regulatory - Export controls, antitrust scrutiny\")\n",
    "    print(\"  6. Technology - Rapid obsolescence, R&D execution risk\")\n",
    "    print(\"  7. Customer Concentration - Large cloud providers represent significant revenue\")\n",
    "else:\n",
    "    print(\"SEC risk factor analysis not available.\")\n",
    "    print(\"This may require SEC API credentials or the filing may not be accessible.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Risk Factor Interpretation\n",
    "\n",
    "**Severity Analysis**: The distribution of risk severity scores reveals which disclosures management considers most material. High-severity risks warrant close monitoring as they have outsized impact potential.\n",
    "\n",
    "**Thematic Trends**: \n",
    "- **Operational risks** dominate, reflecting complexity of semiconductor manufacturing\n",
    "- **Geopolitical risks** have increased in recent filings (China export controls, CHIPS Act)\n",
    "- **Competitive risks** acknowledge threat from hyperscalers developing custom silicon\n",
    "\n",
    "**Quarter-over-Quarter Changes** (when available):\n",
    "- New risk factors indicate emerging concerns\n",
    "- Removal of risks may signal resolved issues\n",
    "- Changes in risk language/emphasis reveal management focus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Macroeconomic Context\n",
    "\n",
    "Semiconductor companies are highly cyclical. We examine key macro indicators to assess the current position in the cycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Macro Dashboard\n",
    "try:\n",
    "    print(\"MACROECONOMIC INDICATORS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    macro_data = get_macro_indicators(settings)\n",
    "    \n",
    "    if macro_data and not macro_data.empty:\n",
    "        # Display latest values\n",
    "        print(\"\\nLatest Values:\")\n",
    "        latest = macro_data.iloc[-1]\n",
    "        for col in macro_data.columns:\n",
    "            print(f\"  â€¢ {col}: {latest[col]:.2f}\")\n",
    "        \n",
    "        # Plot macro trends\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        for idx, col in enumerate(macro_data.columns[:4]):\n",
    "            ax = axes[idx // 2, idx % 2]\n",
    "            ax.plot(macro_data.index, macro_data[col], linewidth=2)\n",
    "            ax.set_title(col, fontweight='bold')\n",
    "            ax.set_xlabel('Date')\n",
    "            ax.grid(alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"\\nðŸ’¡ MACRO IMPLICATIONS FOR SEMICONDUCTORS:\")\n",
    "        print(\"  â€¢ Interest rates affect WACC and valuation multiples\")\n",
    "        print(\"  â€¢ GDP growth correlates with semiconductor demand\")\n",
    "        print(\"  â€¢ Industrial production signals capex cycles\")\n",
    "        print(\"  â€¢ Leading indicators help time cyclical positions\")\n",
    "    else:\n",
    "        print(\"Macro data not available. Check FRED API key in settings.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error fetching macro data: {e}\")\n",
    "    print(\"This feature requires a FRED API key.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Investment Thesis & Price Target\n",
    "\n",
    "Synthesizing all analyses into actionable investment recommendation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investment Thesis Summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"INVESTMENT THESIS: NVIDIA (NVDA)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate price targets\n",
    "if dcf_result:\n",
    "    base_target = dcf_result.equity_value_per_share\n",
    "    bull_target = base_target * 1.25  # 25% above base case\n",
    "    bear_target = base_target * 0.75  # 25% below base case\n",
    "    \n",
    "    print(\"\\nðŸ’° PRICE TARGETS:\")\n",
    "    print(f\"  â€¢ Current Price: ${current_price:.2f}\")\n",
    "    print(f\"  â€¢ Bear Case: ${bear_target:.2f} ({(bear_target/current_price-1)*100:+.1f}%)\")\n",
    "    print(f\"  â€¢ Base Case: ${base_target:.2f} ({(base_target/current_price-1)*100:+.1f}%)\")\n",
    "    print(f\"  â€¢ Bull Case: ${bull_target:.2f} ({(bull_target/current_price-1)*100:+.1f}%)\")\n",
    "    \n",
    "    # Determine recommendation\n",
    "    upside = (base_target / current_price - 1) * 100\n",
    "    if upside > 20:\n",
    "        recommendation = \"STRONG BUY\"\n",
    "        rating = 5\n",
    "    elif upside > 10:\n",
    "        recommendation = \"BUY\"\n",
    "        rating = 4\n",
    "    elif upside > -10:\n",
    "        recommendation = \"HOLD\"\n",
    "        rating = 3\n",
    "    elif upside > -20:\n",
    "        recommendation = \"REDUCE\"\n",
    "        rating = 2\n",
    "    else:\n",
    "        recommendation = \"SELL\"\n",
    "        rating = 1\n",
    "    \n",
    "    print(f\"\\nðŸŽ¯ RECOMMENDATION: {recommendation} ({rating}/5)\")\n",
    "    print(f\"   Implied Upside: {upside:+.1f}%\")\n",
    "\n",
    "print(\"\\nðŸ“ˆ BULL CASE ARGUMENTS:\")\n",
    "print(\"  1. AI Secular Growth: Multi-year AI infrastructure build-out driving GPU demand\")\n",
    "print(\"  2. Pricing Power: Limited competition in high-end AI accelerators enables premium pricing\")\n",
    "print(\"  3. Software Moat: CUDA ecosystem creates high switching costs\")\n",
    "print(\"  4. Market Share Gains: Taking share in data center from CPUs, expanding TAM\")\n",
    "print(\"  5. Product Cycle: Blackwell architecture launch in 2024-25 provides growth catalyst\")\n",
    "print(\"  6. Margin Expansion: Operating leverage from scale, mix shift to data center\")\n",
    "\n",
    "print(\"\\nðŸ“‰ BEAR CASE ARGUMENTS:\")\n",
    "print(\"  1. Valuation: Trading at premium multiples leaves little room for disappointment\")\n",
    "print(\"  2. Competition: AMD gaining share, hyperscaler custom chips (TPU, Trainium, Inferentia)\")\n",
    "print(\"  3. Cyclicality: AI infrastructure spend could normalize or retrench\")\n",
    "print(\"  4. Geopolitical: China export restrictions, Taiwan supply risk\")\n",
    "print(\"  5. Customer Concentration: Top 4 customers ~40% of revenue, negotiating leverage\")\n",
    "print(\"  6. Regulatory: Antitrust scrutiny on market dominance\")\n",
    "\n",
    "print(\"\\nðŸŽ² KEY RISKS TO MONITOR:\")\n",
    "print(\"  â€¢ Quarterly revenue guidance - indicator of AI demand sustainability\")\n",
    "print(\"  â€¢ Gross margin trajectory - competition/pricing pressure signal\")\n",
    "print(\"  â€¢ Data center growth rate - core growth driver\")\n",
    "print(\"  â€¢ Hyperscaler capex guidance - validates AI infrastructure thesis\")\n",
    "print(\"  â€¢ Geopolitical developments - Taiwan, China export controls\")\n",
    "print(\"  â€¢ Competitive product launches - AMD MI300, Intel Gaudi, custom chips\")\n",
    "\n",
    "print(\"\\nâ±ï¸  INVESTMENT HORIZON:\")\n",
    "print(\"  â€¢ Short-term (3-6 months): Volatile, dependent on earnings and macro\")\n",
    "print(\"  â€¢ Medium-term (1-2 years): Strong fundamentals, AI tailwinds continue\")\n",
    "print(\"  â€¢ Long-term (3-5 years): Execution risk increases, competition intensifies\")\n",
    "\n",
    "print(\"\\nðŸ’¼ POSITION SIZING GUIDANCE:\")\n",
    "if risk_result and 'historical' in risk_result.var_results:\n",
    "    var_95 = abs(risk_result.var_results['historical'].get('var_95', 5))\n",
    "    print(f\"  â€¢ Given 95% VaR of {var_95:.1f}%, consider position size limits\")\n",
    "    print(f\"  â€¢ For $100k portfolio: Max position of ${100000 / (var_95 * 5):.0f} ({100 / (var_95 * 5):.1f}%)\")\n",
    "print(\"  â€¢ High volatility argues for dollar-cost averaging vs lump-sum\")\n",
    "print(\"  â€¢ Consider hedging strategies (covered calls, protective puts) for large positions\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"END OF REPORT\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nGenerated by Strategic Alpha Suite on {ANALYSIS_DATE}\")\n",
    "print(\"For informational purposes only. Not investment advice.\")\n",
    "print(\"\\nNext Steps:\")\n",
    "print(\"  1. Monitor Q4 2024 earnings call (Feb 2025)\")\n",
    "print(\"  2. Track Blackwell ramp and customer feedback\")\n",
    "print(\"  3. Watch competitive dynamics (AMD, hyperscaler silicon)\")\n",
    "print(\"  4. Re-run analysis quarterly to update thesis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Export to PDF\n",
    "\n",
    "To share this analysis, export to PDF:\n",
    "\n",
    "```bash\n",
    "# Install nbconvert if needed\n",
    "pip install nbconvert\n",
    "\n",
    "# Export to PDF\n",
    "jupyter nbconvert --to pdf nvda_deep_dive.ipynb\n",
    "\n",
    "# Or export to HTML first (easier, no LaTeX dependencies)\n",
    "jupyter nbconvert --to html nvda_deep_dive.ipynb\n",
    "```\n",
    "\n",
    "**Alternative**: Use Jupyter's built-in \"File > Download as > PDF\" option."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Appendix: Methodology Notes\n",
    "\n",
    "### DCF Model\n",
    "- **Projection Period**: 10 years\n",
    "- **Terminal Growth Rate**: 2.5% (slightly above GDP)\n",
    "- **WACC Calculation**: CAPM beta, market risk premium 7%, risk-free rate from 10-year Treasury\n",
    "- **Tax Rate**: Effective tax rate from historical financials\n",
    "\n",
    "### VaR Methodology\n",
    "- **Historical VaR**: Based on empirical return distribution (non-parametric)\n",
    "- **Confidence Levels**: 95% and 99%\n",
    "- **Time Horizon**: 1-day returns (can scale to multi-day)\n",
    "\n",
    "### Supply Chain Analysis\n",
    "- **Network Metrics**: Betweenness centrality, degree centrality\n",
    "- **Chokepoint Definition**: Nodes with high betweenness and low redundancy\n",
    "- **Data Sources**: Public supplier relationships, industry reports\n",
    "\n",
    "### SEC Risk Scoring\n",
    "- **Severity Score**: 0-10 scale based on keyword frequency and linguistic patterns\n",
    "- **Categories**: Operational, Financial, Regulatory, Competitive, Geopolitical\n",
    "- **Limitations**: Automated scoring may not capture all nuances\n",
    "\n",
    "---\n",
    "\n",
    "**Disclaimer**: This analysis is for educational and informational purposes only. It does not constitute investment advice, a recommendation to buy or sell securities, or an offer of securities. Past performance does not guarantee future results. Investing involves risk, including possible loss of principal. Consult a qualified financial advisor before making investment decisions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
